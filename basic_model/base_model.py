# -*- coding: utf-8 -*-
"""
Evaluate multimodal MCQs (text + optional image) using Qwen2-VL.

Input CSV: ../sample_questions/NLP_HW3_QUESTIONS.csv
  Columns (Persian): Ø³ÙˆØ§Ù„, Ú¯Ø²ÛŒÙ†Ù‡ ÛŒÚ©, Ú¯Ø²ÛŒÙ†Ù‡ Ø¯Ùˆ, Ú¯Ø²ÛŒÙ†Ù‡ Ø³Ù‡, Ú¯Ø²ÛŒÙ†Ù‡ Ú†Ù‡Ø§Ø±, Ù¾Ø§Ø³Ø® Ø¯Ø±Ø³Øª, Ø¹Ú©Ø³, ØªØ±Ú©ÛŒØ¨ Ø³ÙˆØ§Ù„ Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ Ù‡Ø§, ...
We ONLY use the fully-composed prompt from 'ØªØ±Ú©ÛŒØ¨ Ø³ÙˆØ§Ù„ Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ Ù‡Ø§'.
If 'Ø¹Ú©Ø³' has a valid path, we include that image.

Output CSV: predictions_qwen2vl.csv
  Columns: row_id, question_used, image_used, option1..4, gold_index, pred_index, pred_label, is_correct, model_output_raw

Model (default): Qwen/Qwen2-VL-2B-Instruct
You can override with --model <hf_repo_id>

Usage:
    python eval_multimodal_mcq.py
    python eval_multimodal_mcq.py --model Qwen/Qwen2-VL-7B-Instruct
"""

import argparse
import re
from pathlib import Path
from typing import List, Optional, Tuple

import pandas as pd
from PIL import Image

import torch
from transformers import AutoProcessor
from transformers import Qwen2VLForConditionalGeneration

# ---------------------- Config ----------------------
DEFAULT_MODEL = "Qwen/Qwen2-VL-2B-Instruct"
INPUT_CSV = Path("../sample_questions/NLP_HW3_QUESTIONS.csv")
OUTPUT_CSV = Path("predictions_qwen2vl.csv")
MAX_NEW_TOKENS = 32
TEMPERATURE = 0.0  # Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø® Ù‚Ø·Ø¹ÛŒ (ÙÙ‚Ø· 1..4)
# ---------------------------------------------------

def smart_open_image(path_str: str) -> Optional[Image.Image]:
    if not isinstance(path_str, str) or not path_str.strip():
        return None
    p = Path(path_str.strip().strip('"').strip("'"))
    # Ø§Ú¯Ø± Ù…Ø³ÛŒØ± Ù†Ø³Ø¨ÛŒ Ø§Ø³ØªØŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø­Ù„ CSV ÙˆØ±ÙˆØ¯ÛŒ Ù‡Ù… Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†ÛŒÙ…
    if not p.exists():
        # ØªÙ„Ø§Ø´ Ø¯ÙˆÙ…: Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ù†Ø³Ø¨ÛŒ Ø¨Ø§Ø´Ø¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù¾ÙˆØ´Ù‡ CSV
        p2 = (INPUT_CSV.parent / p).resolve()
        if p2.exists():
            p = p2
    if not p.exists():
        return None
    try:
        return Image.open(p).convert("RGB")
    except Exception:
        return None

def extract_digit_1_to_4(text: str) -> Optional[int]:
    """
    Ø§Ø² Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ØŒ Ø§ÙˆÙ„ÛŒÙ† Ø¹Ø¯Ø¯ 1..4 (Ù„Ø§ØªÛŒÙ† ÛŒØ§ ÙØ§Ø±Ø³ÛŒ) Ø±Ø§ Ø¨ÛŒØ±ÙˆÙ† Ù…ÛŒâ€ŒÚ©Ø´Ø¯.
    """
    if not text:
        return None
    # Ø§Ø¹Ø¯Ø§Ø¯ ÙØ§Ø±Ø³ÛŒ Ø¨Ù‡ Ù„Ø§ØªÛŒÙ†
    fa_to_en = str.maketrans("Û°Û±Û²Û³Û´ÛµÛ¶Û·Û¸Û¹", "0123456789")
    t = text.translate(fa_to_en)
    m = re.search(r"\b([1-4])\b", t)
    if m:
        return int(m.group(1))
    # Ú¯Ø§Ù‡ÛŒ Ù…Ø¯Ù„ Ù…ÛŒâ€ŒÙ†ÙˆÛŒØ³Ø¯: "Ú¯Ø²ÛŒÙ†Ù‡ 3" ÛŒØ§ "Answer: 2"
    m = re.search(r"(?:Ú¯Ø²ÛŒÙ†Ù‡|option|answer|ans)\D*([1-4])", t, re.I)
    if m:
        return int(m.group(1))
    return None

def pick_by_fuzzy_match(model_text: str, options: List[str]) -> Optional[int]:
    """
    Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø¨Ù‡â€ŒØ¬Ø§ÛŒ Ø¹Ø¯Ø¯ØŒ Ù†Ø§Ù… Ú¯Ø²ÛŒÙ†Ù‡ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯ØŒ Ø¨Ø§ ØªØ·Ø¨ÛŒÙ‚ Ø³Ø§Ø¯Ù‡ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ±ÛŒÙ† Ú¯Ø²ÛŒÙ†Ù‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
    """
    from difflib import SequenceMatcher
    if not model_text:
        return None
    scores = []
    for i, opt in enumerate(options, start=1):
        ratio = SequenceMatcher(None, model_text.strip(), opt.strip()).ratio()
        scores.append((ratio, i))
    scores.sort(reverse=True)
    best_ratio, best_idx = scores[0]
    return best_idx if best_ratio >= 0.4 else None

def build_messages(full_prompt_text: str, img: Optional[Image.Image]) -> Tuple[list, list]:
    """
    Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú†ØªÛŒ Ù…Ø®ØµÙˆØµ Qwen2-VL Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
    ÙÙ‚Ø· Ø§Ø² 'ØªØ±Ú©ÛŒØ¨ Ø³ÙˆØ§Ù„ Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ Ù‡Ø§' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ùˆ Ø§Ø² Ù…Ø¯Ù„ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… ÛŒÚ© Ø¹Ø¯Ø¯ 1..4 Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.
    """
    system_text = (
        "ØªÙˆ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù¾Ø§Ø³Ø®â€ŒÚ¯ÙˆÛŒ Ú†Ù†Ø¯Ú¯Ø²ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒ. "
        "ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ø§ ÛŒÚ© Ø¹Ø¯Ø¯ Ø§Ø² 1 ØªØ§ 4 Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡Ø› Ù‡ÛŒÚ† Ù…ØªÙ† Ø¯ÛŒÚ¯Ø±ÛŒ Ù†Ù†ÙˆÛŒØ³."
    )

    user_suffix = "\n\nÙ„Ø·ÙØ§Ù‹ ÙÙ‚Ø· Ø¨Ø§ ÛŒÚ© Ø¹Ø¯Ø¯ Ø§Ø² Û± ØªØ§ Û´ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù‡."
    content = []
    images = []
    if img is not None:
        content.append({"type": "image", "image": img})
        images.append(img)
    content.append({"type": "text", "text": full_prompt_text + user_suffix})

    messages = [
        {"role": "system", "content": [{"type": "text", "text": system_text}]},
        {"role": "user", "content": content},
    ]
    return messages, images

def run_row(processor, model, device, row_id: int, row: pd.Series) -> dict:
    # Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒÙ…
    # Ø§Ø¬Ø¨Ø§Ø±ÛŒ: Â«ØªØ±Ú©ÛŒØ¨ Ø³ÙˆØ§Ù„ Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ Ù‡Ø§Â»
    possible_combo_cols = [c for c in row.index if "ØªØ±Ú©ÛŒØ¨" in str(c)]
    if not possible_combo_cols:
        raise KeyError("Ø³ØªÙˆÙ† Â«ØªØ±Ú©ÛŒØ¨ Ø³ÙˆØ§Ù„ Ùˆ Ú¯Ø²ÛŒÙ†Ù‡ Ù‡Ø§Â» Ø¯Ø± CSV Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯.")
    combo_col = possible_combo_cols[0]
    full_prompt_text = str(row[combo_col]).strip()

    # Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ùˆ Ù¾Ø§Ø³Ø® Ø·Ù„Ø§ÛŒÛŒ
    opt_cols = [c for c in row.index if "Ú¯Ø²ÛŒÙ†Ù‡" in str(c)]
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ ÛŒÚ©..Ú†Ù‡Ø§Ø±
    name_to_idx = {"ÛŒÚ©":1, "Ø¯Ùˆ":2, "Ø³Ù‡":3, "Ú†Ù‡Ø§Ø±":4}
    def opt_key(cname):
        for k, v in name_to_idx.items():
            if k in str(cname):
                return v
        # fallback: Ø³Ø¹ÛŒ Ú©Ù† Ø§Ù†ØªÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ø¨ÛŒØ§Ø¨ÛŒ
        m = re.search(r"(\d+)$", str(cname))
        return int(m.group(1)) if m else 999
    opt_cols = sorted(opt_cols, key=opt_key)[:4]
    options = [str(row[c]).strip() for c in opt_cols]

    # Ù¾Ø§Ø³Ø® Ø¯Ø±Ø³Øª (Ø´Ù…Ø§Ø±Ù‡ 1..4)
    gold_idx = None
    for key in row.index:
        if "Ù¾Ø§Ø³Ø®" in str(key):
            try:
                gold_idx = int(str(row[key]).strip().replace(" ", ""))
            except Exception:
                pass
            break

    # ØªØµÙˆÛŒØ± (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)
    img = None
    for key in row.index:
        if "Ø¹Ú©Ø³" in str(key):
            img = smart_open_image(row[key])
            break

    # Ø³Ø§Ø®Øª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„
    messages, images = build_messages(full_prompt_text, img)

    # Qwen2-VL chat template -> inputs
    prompt = processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    inputs = processor(
        text=prompt,
        images=images if images else None,
        return_tensors="pt"
    ).to(device)

    with torch.no_grad():
        generated = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=(TEMPERATURE > 0),
            temperature=TEMPERATURE,
            pad_token_id=processor.tokenizer.eos_token_id
        )

    # ÙÙ‚Ø· Ø¨Ø®Ø´ ØªÙˆÙ„ÛŒØ¯ÛŒ Ø±Ø§ Ø¨Ú¯ÛŒØ±ÛŒÙ…
    gen_ids = generated[:, inputs["input_ids"].shape[1]:]
    output_text = processor.batch_decode(gen_ids, skip_special_tokens=True)[0].strip()

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø§Ø³Ø® 1..4
    pred_idx = extract_digit_1_to_4(output_text)
    if pred_idx is None:
        pred_idx = pick_by_fuzzy_match(output_text, options)

    pred_label = options[pred_idx - 1] if pred_idx and 1 <= pred_idx <= 4 else ""

    return {
        "row_id": row_id,
        "question_used": full_prompt_text,
        "image_used": "yes" if img is not None else "no",
        "option1": options[0] if len(options) > 0 else "",
        "option2": options[1] if len(options) > 1 else "",
        "option3": options[2] if len(options) > 2 else "",
        "option4": options[3] if len(options) > 3 else "",
        "gold_index": gold_idx,
        "pred_index": pred_idx,
        "pred_label": pred_label,
        "is_correct": (pred_idx == gold_idx) if (pred_idx is not None and gold_idx is not None) else None,
        "model_output_raw": output_text,
    }

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default=DEFAULT_MODEL, help="HF repo id, e.g., Qwen/Qwen2-VL-2B-Instruct")
    parser.add_argument("--input", type=str, default=str(INPUT_CSV), help="Path to input CSV")
    parser.add_argument("--output", type=str, default=str(OUTPUT_CSV), help="Path to output CSV")
    args = parser.parse_args()

    input_csv = Path(args.input)
    output_csv = Path(args.output)

    if not input_csv.exists():
        raise FileNotFoundError(f"CSV not found: {input_csv}")

    print(f"ğŸ“¥ Loading CSV: {input_csv}")
    # utf-8-sig Ø¨Ø±Ø§ÛŒ BOM Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
    df = pd.read_csv(input_csv, encoding="utf-8-sig", engine="python")

    # Device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ§  Loading model: {args.model} on {device}")
    model = Qwen2VLForConditionalGeneration.from_pretrained(
        args.model,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None
    )
    processor = AutoProcessor.from_pretrained(args.model)

    results = []
    for i, row in df.iterrows():
        try:
            res = run_row(processor, model, device, i, row)
        except Exception as e:
            res = {
                "row_id": i, "question_used": "", "image_used": "",
                "option1":"", "option2":"", "option3":"", "option4":"",
                "gold_index": None, "pred_index": None, "pred_label": "",
                "is_correct": None, "model_output_raw": f"ERROR: {e}"
            }
        results.append(res)
        if (i+1) % 5 == 0:
            print(f"â€¦ processed {i+1} rows")

    out_df = pd.DataFrame(results)
    out_df.to_csv(output_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… Saved predictions to: {output_csv} | {len(out_df)} rows")

if __name__ == "__main__":
    main()
